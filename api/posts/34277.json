{"title":"Python 中文分词及词频统计","slug":"Python中文分词及词频统计","date":"2020-10-03","updated":"2020-10-03","comments":true,"path":"api/posts/34277.json","excerpt":null,"cover":"https://upload-images.jianshu.io/upload_images/1428402-028a113cccb9b77d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1127/format/webp","covers":["https://upload-images.jianshu.io/upload_images/1428402-028a113cccb9b77d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1127/format/webp","https://upload-images.jianshu.io/upload_images/1428402-719a1d31198ac833.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/818/format/webp","https://upload-images.jianshu.io/upload_images/1428402-93ddad1eb99e5ccc.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1200/format/webp","https://upload-images.jianshu.io/upload_images/1428402-05547e7258b9a1a2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/192/format/webp","https://upload-images.jianshu.io/upload_images/1428402-3e9658f5c5c3c476.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1200/format/webp"],"content":"<h1 id=\"Python 中文分词及词频统计\"><a href=\"#Python 中文分词及词频统计\" class=\"headerlink\" title=\"Python 中文分词及词频统计\"></a>Python 中文分词及词频统计</h1><p><img src=\"https://upload-images.jianshu.io/upload_images/1428402-028a113cccb9b77d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1127/format/webp\"></p>\n<h2 id=\"中文分词\"><a href=\"# 中文分词\" class=\"headerlink\" title=\"中文分词\"></a>中文分词 </h2><p> 中文分词(Chinese Word Segmentation)，将中文语句切割成单独的词组。英文使用空格来分开每个单词的，而中文单独一个汉字跟词有时候完全不是同个含义，因此，中文分词相比英文分词难度高很多。</p>\n<p>分词主要用于 NLP 自然语言处理（Natural Language Processing），使用场景有：</p>\n<p>搜索优化，关键词提取（百度指数）<br>语义分析，智能问答系统（客服系统）<br>非结构化文本媒体内容，如社交信息（微博热榜）<br>文本聚类，根据内容生成分类（行业分类）</p>\n<h3 id=\"分词库\"><a href=\"# 分词库\" class=\"headerlink\" title=\"分词库\"></a>分词库</h3><p>Python 的中文分词库有很多，常见的有：</p>\n<ul>\n<li>jieba（结巴分词）</li>\n<li>THULAC（清华大学自然语言处理与社会人文计算实验室）</li>\n<li>pkuseg（北京大学语言计算与机器学习研究组）</li>\n<li>SnowNLP</li>\n<li>pynlpir</li>\n<li>CoreNLP</li>\n<li>pyltp</li>\n</ul>\n<p>通常前三个是比较经常见到的，主要在易用性 / 准确率 / 性能都还不错。我个人常用的一直都是结巴分词（比较早接触），最近使用 pkuseg，两者的使用后面详细讲。</p>\n<h2 id=\"结巴分词\"><a href=\"# 结巴分词\" class=\"headerlink\" title=\"结巴分词\"></a>结巴分词 </h2><h3 id=\"简介\"><a href=\"# 简介\" class=\"headerlink\" title=\"简介\"></a> 简介</h3><ul>\n<li>“结巴”中文分词：做最好的 Python 中文分词组件</li>\n<li>支持三种分词模式：<ul>\n<li>精确模式，试图将句子最精确地切开，适合文本分析；</li>\n<li>全模式，把句子中所有的可以成词的词语都扫描出来, 速度非常快，但是不能解决歧义；</li>\n<li>搜索引擎模式，在精确模式的基础上，对长词再次切分，提高召回率，适合用于搜索引擎分词。</li>\n</ul>\n</li>\n<li>支持繁体分词</li>\n<li>支持自定义词典</li>\n</ul>\n<h3 id=\"实例\"><a href=\"# 实例\" class=\"headerlink\" title=\"实例\"></a>实例 </h3><p> 我们使用京东商场的美的电器评论来看看结巴分词的效果。如果你没有安装结巴分词库则需要在命令行下输入 pip install jieba，安装完之后即可开始分词之旅。</p>\n<p>评论数据整理在文件 meidi_jd.csv 文件中，读取数据前先导入相关库。因为中文的文本或文件的编码方式不同编码选择 gb18030，有时候是 utf-8、gb2312、gbk 自行测试。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 导入相关库</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> pandas <span class=\"keyword\">as</span> pd</span><br><span class=\"line\"><span class=\"keyword\">import</span> jieba</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 读取数据</span></span><br><span class=\"line\">data = pd.read_csv(<span class=\"string\">&#x27;meidi_jd.csv&#x27;</span>, encoding=<span class=\"string\">&#x27;gb18030&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 查看数据</span></span><br><span class=\"line\">data.head()</span><br></pre></td></tr></table></figure>\n\n<p><img src=\"https://upload-images.jianshu.io/upload_images/1428402-719a1d31198ac833.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/818/format/webp\" alt=\"买家评价\"></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 生成分词</span></span><br><span class=\"line\">data[<span class=\"string\">&#x27;cut&#x27;</span>] = data[<span class=\"string\">&#x27;comment&#x27;</span>].apply(<span class=\"keyword\">lambda</span> x : list(jieba.cut(x)))</span><br><span class=\"line\"></span><br><span class=\"line\">data.head()</span><br></pre></td></tr></table></figure>\n\n<p><img src=\"https://upload-images.jianshu.io/upload_images/1428402-93ddad1eb99e5ccc.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1200/format/webp\" alt=\"分词结果\"></p>\n<p>到这里我们仅仅通过一行代码即可生成中文的分词列表，如果你想要生成分词后去重可以改成这样。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">data[<span class=\"string\">&#x27;cut&#x27;</span>] = data[<span class=\"string\">&#x27;comment&#x27;</span>].apply(<span class=\"keyword\">lambda</span> x : list(set(jieba.cut(x))))</span><br></pre></td></tr></table></figure>\n\n<pre><code>[&#39; 很 &#39;, &#39; 好 &#39;, &#39; 很 &#39;, &#39; 好 &#39;, &#39; 很 &#39;, &#39; 好 &#39;, &#39; 很 &#39;, &#39; 好 &#39;, &#39; 很 &#39;, &#39; 好 &#39;, &#39; 很 &#39;, &#39; 好 &#39;, &#39; 很 &#39;, &#39; 好 &#39;, &#39; 很 &#39;, &#39; 好 &#39;, &#39; 很 &#39;, &#39; 好 &#39;, &#39; 很 &#39;, &#39; 好 &#39;, &#39; 很 &#39;, &#39; 好 &#39;]</code></pre>\n<p>这时候我们就需要导入自定义的词典，以便包含 jieba 词库里没有的词。虽然 jieba 有新词识别能力，但是自行添加新词可以保证更高的正确率。自定义词典采用一词一行，为了演示我添加了“很好”并保存在 dict.txt 文件中，让我们开始用自定义的词典吧！</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">data[<span class=\"string\">&#x27;cut&#x27;</span>] = data[<span class=\"string\">&#x27;comment&#x27;</span>].apply(<span class=\"keyword\">lambda</span> x : list(jieba.cut(x)))</span><br><span class=\"line\"></span><br><span class=\"line\">data.head()</span><br><span class=\"line\"></span><br><span class=\"line\">print(data[<span class=\"string\">&#x27;cut&#x27;</span>].loc[<span class=\"number\">14</span>])</span><br></pre></td></tr></table></figure>\n\n\n<pre><code>[&#39; 很好 &#39;, &#39; 很好 &#39;, &#39; 很好 &#39;, &#39; 很好 &#39;, &#39; 很好 &#39;, &#39; 很好 &#39;, &#39; 很好 &#39;, &#39; 很好 &#39;, &#39; 很好 &#39;, &#39; 很好 &#39;, &#39; 很好 &#39;]</code></pre>\n<h2 id=\"停用词\"><a href=\"# 停用词\" class=\"headerlink\" title=\"停用词\"></a>停用词 </h2><p> 分词的过程中我们会发现实际上有些词实际上意义不大，比如：标点符号、嗯、啊等词，这个时候我们需要将停用词去除掉。首先我们需要有个停用词词组，可以自定义也可以从网上下载词库，这里我们使用网上下载的停用词文件 StopwordsCN.txt。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 读取停用词数据</span></span><br><span class=\"line\">stopwords = pd.read_csv(<span class=\"string\">&#x27;StopwordsCN.txt&#x27;</span>, encoding=<span class=\"string\">&#x27;utf8&#x27;</span>, names=[<span class=\"string\">&#x27;stopword&#x27;</span>], index_col=<span class=\"literal\">False</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">stopwords.head()</span><br></pre></td></tr></table></figure>\n\n<p><img src=\"https://upload-images.jianshu.io/upload_images/1428402-05547e7258b9a1a2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/192/format/webp\" alt=\"停用词\"></p>\n<p>接下里我们只要适当更改分词的代码即可在分词的时候去掉停用词：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 转化词列表</span></span><br><span class=\"line\">stop_list = stopwords[<span class=\"string\">&#x27;stopword&#x27;</span>].tolist()</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 去除停用词</span></span><br><span class=\"line\">data[<span class=\"string\">&#x27;cut&#x27;</span>] = data[<span class=\"string\">&#x27;comment&#x27;</span>].apply(<span class=\"keyword\">lambda</span> x : [i <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> jieba.cut(x) <span class=\"keyword\">if</span> i <span class=\"keyword\">not</span> <span class=\"keyword\">in</span> stop_list])</span><br><span class=\"line\"></span><br><span class=\"line\">data.head()</span><br></pre></td></tr></table></figure>\n\n<p><img src=\"https://upload-images.jianshu.io/upload_images/1428402-3e9658f5c5c3c476.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1200/format/webp\" alt=\"去停用词\"></p>\n<h2 id=\"词频统计\"><a href=\"# 词频统计\" class=\"headerlink\" title=\"词频统计\"></a>词频统计 </h2><p> 到这里我们基本是已经学会用 Python 库进行分词，关于词频统计的方式也很多，我们先将所有分词合并在一起方便统计。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 将所有的分词合并</span></span><br><span class=\"line\">words = []</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">for</span> content <span class=\"keyword\">in</span> data[<span class=\"string\">&#x27;cut&#x27;</span>]:</span><br><span class=\"line\">    words.extend(content)</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"方式一：\"><a href=\"# 方式一：\" class=\"headerlink\" title=\"方式一：\"></a>方式一：</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 创建分词数据框</span></span><br><span class=\"line\">corpus = pd.DataFrame(words, columns=[<span class=\"string\">&#x27;word&#x27;</span>])</span><br><span class=\"line\">corpus[<span class=\"string\">&#x27;cnt&#x27;</span>] = <span class=\"number\">1</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 分组统计</span></span><br><span class=\"line\">g = corpus.groupby([<span class=\"string\">&#x27;word&#x27;</span>]).agg(&#123;<span class=\"string\">&#x27;cnt&#x27;</span>: <span class=\"string\">&#x27;count&#x27;</span>&#125;).sort_values(<span class=\"string\">&#x27;cnt&#x27;</span>, ascending=<span class=\"literal\">False</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">g.head(<span class=\"number\">10</span>)</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"方式二：\"><a href=\"# 方式二：\" class=\"headerlink\" title=\"方式二：\"></a>方式二：</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 导入相关库</span></span><br><span class=\"line\"><span class=\"keyword\">from</span> collections <span class=\"keyword\">import</span> Counter</span><br><span class=\"line\"><span class=\"keyword\">from</span> pprint <span class=\"keyword\">import</span> pprint</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">counter = Counter(words)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 打印前十高频词</span></span><br><span class=\"line\">pprint(counter.most_common(<span class=\"number\">10</span>))</span><br></pre></td></tr></table></figure>\n\n\n<pre><code>[(&#39; 不错 &#39;, 3913),\n(&#39; 安装 &#39;, 3055),\n(&#39; 好 &#39;, 2045),\n(&#39; 很好 &#39;, 1824),\n(&#39; 买 &#39;, 1634),\n(&#39; 热水器 &#39;, 1182),\n(&#39; 挺 &#39;, 1051),\n(&#39; 师傅 &#39;, 923),\n(&#39; 美 &#39;, 894),\n(&#39; 送货 &#39;, 821)]</code></pre>\n<h2 id=\"结尾\"><a href=\"# 结尾\" class=\"headerlink\" title=\"结尾\"></a>结尾 </h2><p> 我个人的使用建议，如果想简单快速上手分词可以使用结巴分词，但如果追求准确度和特定领域分词可以选择 pkuseg 加载模型再分词。另外 jieba 和 THULAC 并没有提供细分领域预训练模型，如果想使用自定义模型分词需使用它们提供的训练接口在细分领域的数据集上进行训练，用训练得到的模型进行中文分词。</p>\n<h2 id=\"代码练习\"><a href=\"# 代码练习\" class=\"headerlink\" title=\"代码练习\"></a>代码练习</h2><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br><span class=\"line\">101</span><br><span class=\"line\">102</span><br><span class=\"line\">103</span><br><span class=\"line\">104</span><br><span class=\"line\">105</span><br><span class=\"line\">106</span><br><span class=\"line\">107</span><br><span class=\"line\">108</span><br><span class=\"line\">109</span><br><span class=\"line\">110</span><br><span class=\"line\">111</span><br><span class=\"line\">112</span><br><span class=\"line\">113</span><br><span class=\"line\">114</span><br><span class=\"line\">115</span><br><span class=\"line\">116</span><br><span class=\"line\">117</span><br><span class=\"line\">118</span><br><span class=\"line\">119</span><br><span class=\"line\">120</span><br><span class=\"line\">121</span><br><span class=\"line\">122</span><br><span class=\"line\">123</span><br><span class=\"line\">124</span><br><span class=\"line\">125</span><br><span class=\"line\">126</span><br><span class=\"line\">127</span><br><span class=\"line\">128</span><br><span class=\"line\">129</span><br><span class=\"line\">130</span><br><span class=\"line\">131</span><br><span class=\"line\">132</span><br><span class=\"line\">133</span><br><span class=\"line\">134</span><br><span class=\"line\">135</span><br><span class=\"line\">136</span><br><span class=\"line\">137</span><br><span class=\"line\">138</span><br><span class=\"line\">139</span><br><span class=\"line\">140</span><br><span class=\"line\">141</span><br><span class=\"line\">142</span><br><span class=\"line\">143</span><br><span class=\"line\">144</span><br><span class=\"line\">145</span><br><span class=\"line\">146</span><br><span class=\"line\">147</span><br><span class=\"line\">148</span><br><span class=\"line\">149</span><br><span class=\"line\">150</span><br><span class=\"line\">151</span><br><span class=\"line\">152</span><br><span class=\"line\">153</span><br><span class=\"line\">154</span><br><span class=\"line\">155</span><br><span class=\"line\">156</span><br><span class=\"line\">157</span><br><span class=\"line\">158</span><br><span class=\"line\">159</span><br><span class=\"line\">160</span><br><span class=\"line\">161</span><br><span class=\"line\">162</span><br><span class=\"line\">163</span><br><span class=\"line\">164</span><br><span class=\"line\">165</span><br><span class=\"line\">166</span><br><span class=\"line\">167</span><br><span class=\"line\">168</span><br><span class=\"line\">169</span><br><span class=\"line\">170</span><br><span class=\"line\">171</span><br><span class=\"line\">172</span><br><span class=\"line\">173</span><br><span class=\"line\">174</span><br><span class=\"line\">175</span><br><span class=\"line\">176</span><br><span class=\"line\">177</span><br><span class=\"line\">178</span><br><span class=\"line\">179</span><br><span class=\"line\">180</span><br><span class=\"line\">181</span><br><span class=\"line\">182</span><br><span class=\"line\">183</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> jieba</span><br><span class=\"line\"><span class=\"keyword\">import</span> jieba.analyse</span><br><span class=\"line\"><span class=\"keyword\">import</span> codecs</span><br><span class=\"line\"><span class=\"keyword\">import</span> re</span><br><span class=\"line\"><span class=\"keyword\">from</span> collections <span class=\"keyword\">import</span> Counter</span><br><span class=\"line\"><span class=\"keyword\">from</span> wordcloud <span class=\"keyword\">import</span> WordCloud</span><br><span class=\"line\"><span class=\"keyword\">import</span> matplotlib.pyplot <span class=\"keyword\">as</span> plt</span><br><span class=\"line\"><span class=\"keyword\">import</span> wordcloud</span><br><span class=\"line\"></span><br><span class=\"line\">str = <span class=\"string\">&quot; 我在河南大学上学，今年 22 岁了 &quot;</span></span><br><span class=\"line\"></span><br><span class=\"line\">seg_list = jieba.cut(str,cut_all=<span class=\"literal\">True</span>)</span><br><span class=\"line\">print(<span class=\"string\">&quot; 全模式：&quot;</span>+<span class=\"string\">&quot;/&quot;</span>.join(seg_list))</span><br><span class=\"line\"></span><br><span class=\"line\">seg_list = jieba.cut(str,cut_all=<span class=\"literal\">False</span>)</span><br><span class=\"line\">print(<span class=\"string\">&quot; 默认(精确模式)：&quot;</span>+<span class=\"string\">&quot;/&quot;</span>.join(seg_list))</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 搜索引擎模式</span></span><br><span class=\"line\">seg_list = jieba.cut_for_search(str)</span><br><span class=\"line\">print(<span class=\"string\">&quot; 搜索引擎模式：&quot;</span>+<span class=\"string\">&quot;/&quot;</span>.join(seg_list))</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 添加用户自定义词典</span></span><br><span class=\"line\">str = <span class=\"string\">&quot; 大连圣亚在大连 &quot;</span></span><br><span class=\"line\">seg_list = jieba.cut(str,cut_all=<span class=\"literal\">False</span>)</span><br><span class=\"line\">print(<span class=\"string\">&quot; 默认(精确模式)：&quot;</span>+<span class=\"string\">&quot;/&quot;</span>.join(seg_list))</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 添加自定义词典后</span></span><br><span class=\"line\">jieba.load_userdict(<span class=\"string\">r&quot;./user_dict.txt&quot;</span>)</span><br><span class=\"line\">seg_list = jieba.cut(str,cut_all=<span class=\"literal\">False</span>)</span><br><span class=\"line\">print(<span class=\"string\">&quot; 默认(精确模式)：&quot;</span>+<span class=\"string\">&quot;/&quot;</span>.join(seg_list))</span><br><span class=\"line\"></span><br><span class=\"line\">str = <span class=\"string\">&quot; 我家在河南省驻马店市汝南县东官庄镇 &quot;</span></span><br><span class=\"line\">seg_list = jieba.cut(str) <span class=\"comment\"># 默认是精确模式</span></span><br><span class=\"line\">print(<span class=\"string\">&quot; 精确模式：&quot;</span>+<span class=\"string\">&quot;/&quot;</span>.join(seg_list))</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 动态调整词典</span></span><br><span class=\"line\">jieba.add_word(<span class=\"string\">&quot; 东官庄镇 &quot;</span>)</span><br><span class=\"line\">seg_list = jieba.cut(str)</span><br><span class=\"line\">print(<span class=\"string\">&quot; 精确模式：&quot;</span>+<span class=\"string\">&quot;/&quot;</span>.join(seg_list))</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 加载停用词</span></span><br><span class=\"line\"><span class=\"comment\"># 创建停用词列表</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">stopwordlist</span>():</span></span><br><span class=\"line\">    stopwords = [line.strip() <span class=\"keyword\">for</span> line <span class=\"keyword\">in</span> open(<span class=\"string\">&#x27;./hit_stopwords.txt&#x27;</span>,encoding=<span class=\"string\">&#x27;UTF-8&#x27;</span>).readlines()]</span><br><span class=\"line\">    <span class=\"keyword\">return</span> stopwords</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 对句子进行中文分词</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">seg_depart</span>(<span class=\"params\">sentence</span>):</span></span><br><span class=\"line\">    print(<span class=\"string\">&#x27; 正在分词 &#x27;</span>)</span><br><span class=\"line\">    sentence_depart = jieba.cut(sentence.strip())</span><br><span class=\"line\">    <span class=\"comment\"># 创建一个停用词列表</span></span><br><span class=\"line\">    stopwords = stopwordlist()</span><br><span class=\"line\">    <span class=\"comment\"># 输出结果为 outstr</span></span><br><span class=\"line\">    outstr = <span class=\"string\">&#x27;&#x27;</span></span><br><span class=\"line\"><span class=\"comment\">#     去停用词</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> word <span class=\"keyword\">in</span> sentence_depart:</span><br><span class=\"line\">        <span class=\"keyword\">if</span> word <span class=\"keyword\">not</span> <span class=\"keyword\">in</span> stopwords:</span><br><span class=\"line\">            <span class=\"keyword\">if</span> word != <span class=\"string\">&#x27;\\t&#x27;</span>:</span><br><span class=\"line\">                outstr += word</span><br><span class=\"line\">                outstr += <span class=\"string\">&quot; &quot;</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> outstr</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 给出文档路径</span></span><br><span class=\"line\">filename = <span class=\"string\">&quot;./text.txt&quot;</span></span><br><span class=\"line\">outfilename = <span class=\"string\">&quot;./out.txt&quot;</span></span><br><span class=\"line\">inputs = open(filename,<span class=\"string\">&#x27;r&#x27;</span>,encoding=<span class=\"string\">&#x27;UTF-8&#x27;</span>)</span><br><span class=\"line\">outputs = open(outfilename,<span class=\"string\">&#x27;w&#x27;</span>,encoding=<span class=\"string\">&#x27;UTF-8&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">jieba.add_word(<span class=\"string\">&quot; 停用词 &quot;</span>)</span><br><span class=\"line\"><span class=\"comment\"># 将输出结果写到 out.txt 中</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> line <span class=\"keyword\">in</span> inputs:</span><br><span class=\"line\">    line_seg = seg_depart(line)</span><br><span class=\"line\"></span><br><span class=\"line\">    outputs.write(line_seg + <span class=\"string\">&#x27;\\n&#x27;</span>)</span><br><span class=\"line\">    print(<span class=\"string\">&quot;------------------------- 正在分词和去停用词 --------------------------&quot;</span>)</span><br><span class=\"line\">    print(<span class=\"string\">&quot; 原文：&quot;</span>+line)</span><br><span class=\"line\">    print(<span class=\"string\">&quot; 去停用词：&quot;</span>+line_seg)</span><br><span class=\"line\"></span><br><span class=\"line\">outputs.close()</span><br><span class=\"line\">inputs.close()</span><br><span class=\"line\">print(<span class=\"string\">&quot; 删除停用词和分词成功！！！&quot;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># class WordCounter(object):</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">count_from_file</span>(<span class=\"params\">file,top_limit=<span class=\"number\">0</span></span>):</span></span><br><span class=\"line\">    <span class=\"keyword\">with</span> codecs.open(file,<span class=\"string\">&#x27;r&#x27;</span>,<span class=\"string\">&#x27;UTF-8&#x27;</span>) <span class=\"keyword\">as</span> f:</span><br><span class=\"line\">        content = f.read()</span><br><span class=\"line\">        <span class=\"comment\"># 将多个空格替换为一个空格</span></span><br><span class=\"line\">        content =re.sub(<span class=\"string\">r&#x27;\\s+&#x27;</span>,<span class=\"string\">r&#x27; &#x27;</span>,content)</span><br><span class=\"line\">        content = re.sub(<span class=\"string\">r&#x27;\\.+&#x27;</span>,<span class=\"string\">r&#x27; &#x27;</span>,content)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># 去停用词</span></span><br><span class=\"line\">        content = seg_depart(content)</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">return</span> count_from_str(content,top_limit=top_limit)</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">count_from_str</span>(<span class=\"params\">content,top_limit=<span class=\"number\">0</span></span>):</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> top_limit &lt;= <span class=\"number\">0</span>:</span><br><span class=\"line\">        top_limit = <span class=\"number\">100</span></span><br><span class=\"line\">    <span class=\"comment\">#     提取文章的关键词</span></span><br><span class=\"line\">    tags = jieba.analyse.extract_tags(content,topK=<span class=\"number\">100</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    words = jieba.cut(content)</span><br><span class=\"line\"></span><br><span class=\"line\">    counter = Counter()</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">for</span> word <span class=\"keyword\">in</span> words:</span><br><span class=\"line\">        <span class=\"keyword\">if</span> word <span class=\"keyword\">in</span> tags:</span><br><span class=\"line\">            counter[word] += <span class=\"number\">1</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">return</span> counter.most_common(top_limit)</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># if __name__ == &#x27;__main__&#x27;:</span></span><br><span class=\"line\"><span class=\"comment\">#     counter = WordCounter()</span></span><br><span class=\"line\"><span class=\"comment\">#     retult = counter.count_from_file(r&#x27;./text.txt&#x27;,top_limit=10)</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># print(retult)</span></span><br><span class=\"line\"></span><br><span class=\"line\">print(<span class=\"string\">&quot; 打印词频 ==============&quot;</span>)</span><br><span class=\"line\">retult = count_from_file(<span class=\"string\">r&#x27;./text.txt&#x27;</span>,top_limit=<span class=\"number\">10</span>)</span><br><span class=\"line\">print(retult)</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">print(<span class=\"string\">&quot;*&quot;</span>*<span class=\"number\">100</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 例子</span></span><br><span class=\"line\"><span class=\"comment\"># sentence = &#x27;Ilikeyou &#x27;</span></span><br><span class=\"line\"><span class=\"comment\"># wc = wordcloud.WordCloud()</span></span><br><span class=\"line\"><span class=\"comment\"># pic = wc.generate(sentence)</span></span><br><span class=\"line\"><span class=\"comment\"># plt.imshow(pic)</span></span><br><span class=\"line\"><span class=\"comment\"># plt.axis(&quot;off&quot;)</span></span><br><span class=\"line\"><span class=\"comment\"># plt.show()</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># wc.to_file(&#x27;test.png&#x27;)</span></span><br><span class=\"line\"></span><br><span class=\"line\">print(<span class=\"string\">&quot; 分词并生成词云图 &quot;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">create_word_cloud</span>(<span class=\"params\">file</span>):</span></span><br><span class=\"line\"></span><br><span class=\"line\">    content = codecs.open(file,<span class=\"string\">&#x27;r&#x27;</span>,<span class=\"string\">&#x27;UTF-8&#x27;</span>).read()</span><br><span class=\"line\"><span class=\"comment\">#     结巴分词</span></span><br><span class=\"line\">    wordlist = jieba.cut(content)</span><br><span class=\"line\">    wl = <span class=\"string\">&quot; &quot;</span>.join(wordlist)</span><br><span class=\"line\"></span><br><span class=\"line\">    print(wl)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#     设置词云图</span></span><br><span class=\"line\">    wc = wordcloud.WordCloud(</span><br><span class=\"line\">    <span class=\"comment\">#     设置背景颜色</span></span><br><span class=\"line\">        background_color=<span class=\"string\">&#x27;pink&#x27;</span>,</span><br><span class=\"line\">    <span class=\"comment\">#     设置最大显示的词数</span></span><br><span class=\"line\">        max_words=<span class=\"number\">100</span>,</span><br><span class=\"line\">    <span class=\"comment\">#     设置字体路径</span></span><br><span class=\"line\">        font_path = <span class=\"string\">&#x27;C:\\Windows\\Fonts\\msyh.ttc&#x27;</span>,</span><br><span class=\"line\">        height = <span class=\"number\">1200</span>,</span><br><span class=\"line\">        width=<span class=\"number\">1600</span>,</span><br><span class=\"line\">    <span class=\"comment\">#     设置字体最大值</span></span><br><span class=\"line\">        max_font_size=<span class=\"number\">300</span>,</span><br><span class=\"line\">    <span class=\"comment\">#     设置有多少种配色方案，即多少种随机生成状态</span></span><br><span class=\"line\">        random_state=<span class=\"number\">30</span>,</span><br><span class=\"line\">    )</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># 生成词云图</span></span><br><span class=\"line\">    myword = wc.generate(wl)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># 展示词云图</span></span><br><span class=\"line\">    plt.imshow(myword)</span><br><span class=\"line\">    plt.axis(<span class=\"string\">&quot;off&quot;</span>)</span><br><span class=\"line\">    plt.show()</span><br><span class=\"line\">    wc.to_file(<span class=\"string\">&#x27;py_pic.png&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># content = codecs.open(&quot;./text.txt&quot;,&#x27;r&#x27;,&#x27;UTF-8&#x27;).read()</span></span><br><span class=\"line\">create_word_cloud(<span class=\"string\">&quot;text.txt&quot;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>","url":"/posts/34277/","min2read":9,"word4post":"2.2k","prev_post":{"title":"免费实用的图床","url":"/posts/6165/"},"next_post":{"title":"测试加密博客","url":"/posts/14783/"},"toc":"<ol class=\"toc\"><li class=\"toc-item toc-level-1\"><a class=\"toc-link\" data-id=\"Python 中文分词及词频统计\" href = \"#\"><span class=\"toc-number\">1.</span> <span class=\"toc-text\">Python 中文分词及词频统计</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" data-id=\"中文分词\" href = \"#\"><span class=\"toc-number\">1.1.</span> <span class=\"toc-text\">中文分词 </span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" data-id=\"分词库\" href = \"#\"><span class=\"toc-number\">1.1.1.</span> <span class=\"toc-text\">分词库</span></a></li></ol></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" data-id=\"结巴分词\" href = \"#\"><span class=\"toc-number\">1.2.</span> <span class=\"toc-text\">结巴分词 </span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" data-id=\"简介\" href = \"#\"><span class=\"toc-number\">1.2.1.</span> <span class=\"toc-text\"> 简介</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" data-id=\"实例\" href = \"#\"><span class=\"toc-number\">1.2.2.</span> <span class=\"toc-text\">实例 </span></a></li></ol></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" data-id=\"停用词\" href = \"#\"><span class=\"toc-number\">1.3.</span> <span class=\"toc-text\">停用词 </span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" data-id=\"词频统计\" href = \"#\"><span class=\"toc-number\">1.4.</span> <span class=\"toc-text\">词频统计 </span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" data-id=\"方式一：\" href = \"#\"><span class=\"toc-number\">1.4.1.</span> <span class=\"toc-text\">方式一：</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" data-id=\"方式二：\" href = \"#\"><span class=\"toc-number\">1.4.2.</span> <span class=\"toc-text\">方式二：</span></a></li></ol></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" data-id=\"结尾\" href = \"#\"><span class=\"toc-number\">1.5.</span> <span class=\"toc-text\">结尾 </span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" data-id=\"代码练习\" href = \"#\"><span class=\"toc-number\">1.6.</span> <span class=\"toc-text\">代码练习</span></a></li></ol></li></ol>","categories":[{"name":"python","path":"api/categories/python.json","url":"/categories/python/"},{"name":"自然语言处理","path":"api/categories/自然语言处理.json","url":"/categories/自然语言处理/"}],"tags":[{"name":"python","path":"api/tags/python.json","url":"/tags/python/"},{"name":"中文分词","path":"api/tags/中文分词.json","url":"/tags/中文分词/"}]}